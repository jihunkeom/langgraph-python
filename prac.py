import asyncio
import json
import logging
import os
import time
import traceback
import uuid
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv
from ibm_watsonx_ai import APIClient, Credentials
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    ToolCall,
    ToolMessage,
)
from langchain_ibm import ChatWatsonx
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent

from config import (
    OPENAI_API_KEY,
    WATSONX_API_KEY,
    WATSONX_PROJECT_ID,
    WATSONX_SPACE_ID,
    WATSONX_URL,
)
from llm_utils import get_llm_stream
from models import (
    AIToolCall,
    ChatCompletionResponse,
    Choice,
    Function,
    Message,
    MessageResponse,
)
from token_utils import get_access_token
from tools import news_search_duckduckgo, tavily_search, web_search_duckduckgo

load_dotenv()

STATE_MODIFIER = "í•­ìƒ í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•˜ì„¸ìš”."
print(WATSONX_URL)

client_model_instance = APIClient(
    credentials=Credentials(url=WATSONX_URL, token=get_access_token(WATSONX_API_KEY)),
    project_id=WATSONX_PROJECT_ID,
)
model_instance = ChatWatsonx(
    model_id="meta-llama/llama-4-maverick-17b-128e-instruct-fp8",
    watsonx_client=client_model_instance,
)
graph = create_react_agent(
    model_instance,
    tools=[news_search_duckduckgo, web_search_duckduckgo],
    prompt=STATE_MODIFIER,
)


# ë°©ë²• 1: ë‹¨ìˆœíˆ ìµœì¢… ê²°ê³¼ë§Œ ë°›ê¸°
def simple_invoke():
    print("=== ë°©ë²• 1: ë‹¨ìˆœ invoke ===")
    response = graph.invoke(
        {"messages": [{"role": "user", "content": "ëŒ€í•œë¯¼êµ­ì˜ ëŒ€í†µë ¹ì€ ëˆ„êµ¬ì¸ê°€ìš”?"}]}
    )
    print("ìµœì¢… ë‹µë³€:")
    print(response["messages"][-1].content)


# ë°©ë²• 2: ì¤‘ìš”í•œ ì´ë²¤íŠ¸ë§Œ í•„í„°ë§í•´ì„œ ì¶œë ¥
async def filtered_stream():
    print("\n=== ë°©ë²• 2: í•„í„°ë§ëœ ìŠ¤íŠ¸ë¦¼ ===")
    async for event in graph.astream_events(
        {"messages": [{"role": "user", "content": "ëŒ€í•œë¯¼êµ­ì˜ ëŒ€í†µë ¹ì€ ëˆ„êµ¬ì¸ê°€ìš”?"}]},
        version="v2",
    ):
        kind = event["event"]
        if kind == "on_chat_model_stream":
            content = event["data"]["chunk"].content
            if content:
                print(content, end="", flush=True)
        elif kind == "on_tool_start":
            tool_name = event.get("name", "")
            print(f"\nğŸ” ë„êµ¬ ì‚¬ìš©: {tool_name}")
        elif kind == "on_tool_end":
            print("âœ… ë„êµ¬ ì™„ë£Œ")


# ë°©ë²• 3: llm_utilsì˜ ìŠ¤íŠ¸ë¦¼ í•¨ìˆ˜ ì‚¬ìš©
async def use_llm_utils():
    print("\n\n=== ë°©ë²• 3: llm_utils ì‚¬ìš© ===")
    messages = [Message(role="user", content="ëŒ€í•œë¯¼êµ­ì˜ ëŒ€í†µë ¹ì€ ëˆ„êµ¬ì¸ê°€ìš”?")]
    model = "meta-llama/llama-4-maverick-17b-128e-instruct-fp8"
    tools = [news_search_duckduckgo, web_search_duckduckgo]

    async for chunk in get_llm_stream(messages, model, "test-thread", tools):
        if chunk.strip():
            # JSON íŒŒì‹±ì„ ì‹œë„í•´ì„œ contentë§Œ ì¶”ì¶œ
            try:
                data = json.loads(
                    chunk.split("data: ")[1] if chunk.startswith("data: ") else chunk
                )
                if "choices" in data and data["choices"]:
                    delta = data["choices"][0].get("delta", {})
                    if "content" in delta:
                        print(delta["content"], end="", flush=True)
            except:
                pass


async def main():
    # ë°©ë²• 1 ì‹¤í–‰
    simple_invoke()

    # ë°©ë²• 2 ì‹¤í–‰
    await filtered_stream()

    # ë°©ë²• 3 ì‹¤í–‰
    await use_llm_utils()


if __name__ == "__main__":
    asyncio.run(main())
